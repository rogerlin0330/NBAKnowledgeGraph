{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `AmpliGraph` to generate GoT Knowledge Graph Embeddings\n",
    "\n",
    "<sub>Content of this notebook was prepared by Basel Shbita (shbita@usc.edu) as part of the class <u>DSCI 558: Building Knowledge Graphs</u> during Fall 2020 at University of Southern California (USC).</sub>\n",
    "\n",
    "**Notes**: \n",
    "- You are supposed to write your code or modify our code in any cell starting with `# ** STUDENT CODE`.\n",
    "- Much content of this notebook was borrowed from AmpliGraph tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AmpliGraph` is a suite of neural machine learning models for relational learning, a branch of machine learning that deals with supervised learning on knowledge graphs. It can be used to <u>generate stand-alone knowledge graph embeddings</u>, discover new knowledge from an existing knowledge graph and complete large knowledge graphs with missing statements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this task, you will gain some hands-on experience working with Knowledge Graph Embeddings. Specifically, you will use the *TransE*, *DistMult* and *ComplEx* models to learn the embeddings of a (small) KG. You will be required to split the dataset to train and test sets, train the model, evaluate it and then generate a visualization for each model type!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the environment\n",
    "\n",
    "Lets install the packages we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ampligraph\n",
    "\n",
    "ampligraph.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the dataset\n",
    "\n",
    "Each relation (i.e. a triple) is in the form:`<subject, predicate, object>`\n",
    "\n",
    "Run the following cell to load the dataset in memory with using the `load_from_csv()` utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.datasets import load_from_csv\n",
    "\n",
    "X = load_from_csv('.', 'similar_players.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the top triples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s</th>\n",
       "      <th>p</th>\n",
       "      <th>o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/players/a/abdelal01.html</td>\n",
       "      <td>has_similar_playing_pattern</td>\n",
       "      <td>/players/b/blounco01.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/players/a/abdelal01.html</td>\n",
       "      <td>has_similar_playing_pattern</td>\n",
       "      <td>/players/b/bryanma01.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/players/a/abdelal01.html</td>\n",
       "      <td>has_similar_playing_pattern</td>\n",
       "      <td>/players/f/fundela01.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/players/a/abdelal01.html</td>\n",
       "      <td>has_similar_playing_pattern</td>\n",
       "      <td>/players/h/hortoed01.html</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/players/a/abdelal01.html</td>\n",
       "      <td>has_similar_playing_pattern</td>\n",
       "      <td>/players/h/hugheri02.html</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           s                            p  \\\n",
       "0  /players/a/abdelal01.html  has_similar_playing_pattern   \n",
       "1  /players/a/abdelal01.html  has_similar_playing_pattern   \n",
       "2  /players/a/abdelal01.html  has_similar_playing_pattern   \n",
       "3  /players/a/abdelal01.html  has_similar_playing_pattern   \n",
       "4  /players/a/abdelal01.html  has_similar_playing_pattern   \n",
       "\n",
       "                           o  \n",
       "0  /players/b/blounco01.html  \n",
       "1  /players/b/bryanma01.html  \n",
       "2  /players/f/fundela01.html  \n",
       "3  /players/h/hortoed01.html  \n",
       "4  /players/h/hugheri02.html  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X, columns=['s', 'p', 'o']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list the subject and object entities found in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/players/a/abdelal01.html', '/players/a/abdulma01.html',\n",
       "       '/players/a/abdulma02.html', ..., '/players/z/zizican01.html',\n",
       "       '/players/z/zopfbi01.html', '/players/z/zubaciv01.html'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = np.unique(np.concatenate([X[:, 0], X[:, 2]]))\n",
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. and all of the relationships that link them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['has_similar_playing_pattern'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relations = np.unique(X[:, 1])\n",
    "relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining train and test datasets\n",
    "\n",
    "As is typical in machine learning, we need to split our dataset into training and test sets.\n",
    "\n",
    "What differs from the standard method of randomly sampling N points to make up our test set, is that our data points are two entities linked by some relationship, and we need to take care to <u>ensure that all entities are represented in train and test sets by at least one triple</u>.\n",
    "\n",
    "To accomplish this, `AmpliGraph` provides the `train_test_split_no_unseen` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.evaluation import train_test_split_no_unseen \n",
    "\n",
    "\n",
    "# we create a 10% test set split\n",
    "X_train, X_test = train_test_split_no_unseen(X, test_size=int(X.shape[0]/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is now split into train/test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size:  (17784, 3)\n",
      "Test set size:  (1976, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Train set size: ', X_train.shape)\n",
    "print('Test set size: ', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1.x.1 Training the model\n",
    "\n",
    "`AmpliGraph` has implemented several Knoweldge Graph Embedding models (*TransE, ComplEx, DistMult, etc...*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.latent_features import TransE, DistMult, ComplEx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets go through the parameters to understand what's going on:\n",
    "- **k**: the dimensionality of the embedding space\n",
    "- **eta ($\\eta$)**: the number of negative, or false triples that must be generated at training runtime for each positive, or true triple\n",
    "- **batches_count**: the number of batches in which the training set is split during the training loop. If you are having into low memory issues than settings this to a higher number may help.\n",
    "- **epochs**: the number of epochs to train the model for.\n",
    "- **optimizer**: the Adam optimizer, with a learning rate of 1e-3 set via the *optimizer_params* kwarg.\n",
    "- **loss**: pairwise loss, with a margin of 0.5 set via the *loss_params* kwarg.\n",
    "- **regularizer**: $L_p$ regularization with $p=2$, i.e. l2 regularization. $\\lambda$ = 1e-5, set via the *regularizer_params* kwarg.\n",
    "\n",
    "Now we can instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** STUDENT CODE \n",
    "# TODO: try different model types: TransE [2.1.1], DistMult [2.1.2], ComplEx [2.1.3]\n",
    "# EmbeddingMethod = TransE\n",
    "# EmbeddingMethod = DistMult\n",
    "EmbeddingMethod = ComplEx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingMethod(batches_count=100, \n",
    "                seed=0, \n",
    "                epochs=200, \n",
    "                k=150, \n",
    "                eta=5,\n",
    "                optimizer='adam', \n",
    "                optimizer_params={'lr':1e-3},\n",
    "                loss='multiclass_nll', \n",
    "                regularizer='LP', \n",
    "                regularizer_params={'p':3, 'lambda':1e-5}, \n",
    "                verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering negatives\n",
    "\n",
    "`AmpliGraph` aims to follow `scikit-learn`'s ease-of-use design philosophy and simplify everything down to `fit`, `evaluate`, and `predict` functions.\n",
    "\n",
    "However, there are some knowledge graph specific steps we must take to ensure our model can be trained and evaluated correctly. The first of these is defining the filter that will be used to ensure that no *negative* statements generated by the corruption procedure are actually positives. This is simply done by concatenating our train and test sets. Now when negative triples are generated by the corruption strategy, we can check that they aren't actually true statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives_filter = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model\n",
    "\n",
    "Once you run the next cell the model will train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Loss:   0.011449: 100%|██████████| 200/200 [05:48<00:00,  1.74s/epoch]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "model.fit(X_train, early_stopping = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.x.2 Evaluating the model\n",
    "\n",
    "Now it's time to evaluate our model on the test set to see how well it's performing.\n",
    "\n",
    "For this we'll use the `evaluate_performance` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.evaluation import evaluate_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's look at the arguments to this function:\n",
    "\n",
    "- `X`: the data to evaluate on. We're going to use our test set to evaluate.\n",
    "- `model`: the model we previously trained.\n",
    "- `filter_triples`: will filter out the false negatives generated by the corruption strategy.\n",
    "- `use_default_protocol`: specifies whether to use the default corruption protocol. If True, then subj and obj are corrupted separately during evaluation.\n",
    "- `verbose`: will give some nice log statements. Let's leave it on for now.\n",
    "\n",
    "Let's run some evaluations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - DeprecationWarning: use_default_protocol will be removed in future. Please use corrupt_side argument instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1976/1976 [00:35<00:00, 55.00it/s]\n"
     ]
    }
   ],
   "source": [
    "ranks = evaluate_performance(X_test, \n",
    "                             model=model, \n",
    "                             filter_triples=positives_filter,\n",
    "                             use_default_protocol=True,\n",
    "                             verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ranks` returned by the `evaluate_performance` function indicate the rank at which the test set triple was found when performing link prediction using the model.\n",
    "\n",
    "<u>For example</u>, if we run the triple `<House Stark of Winterfell, IN_REGION, The North>` and the model returns a rank of `7`, it tells us that while it's not the highest likelihood true statement (which would be given a rank 1), it's pretty likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the evaluation metrics, we are going to use the `mrr_score` (mean reciprocal rank) and `hits_at_n_score` functions:\n",
    "- `mrr_score`: The function computes the mean of the reciprocal of elements of a vector of rankings ranks.\n",
    "- `hits_at_n_score`: The function computes how many elements of a vector of rankings ranks make it to the top n positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR: 0.22\n",
      "Hits@10: 0.48\n",
      "Hits@3: 0.24\n",
      "Hits@1: 0.10\n"
     ]
    }
   ],
   "source": [
    "from ampligraph.evaluation import mr_score, mrr_score, hits_at_n_score\n",
    "\n",
    "mrr = mrr_score(ranks)\n",
    "print(\"MRR: %.2f\" % (mrr))\n",
    "\n",
    "hits_10 = hits_at_n_score(ranks, n=10)\n",
    "print(\"Hits@10: %.2f\" % (hits_10))\n",
    "hits_3 = hits_at_n_score(ranks, n=3)\n",
    "print(\"Hits@3: %.2f\" % (hits_3))\n",
    "hits_1 = hits_at_n_score(ranks, n=1)\n",
    "print(\"Hits@1: %.2f\" % (hits_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Hits@N` indicates how many times in average a true triple was ranked in the top-N. The choice of which N makes more sense depends on the application. The Mean Reciprocal Rank (`MRR`) is another popular metrics to assess the predictive power of a model.\n",
    "\n",
    "**^ Please note that a screenshot of these scores are required for task of 2.1.x.2 ^**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting New Links\n",
    "\n",
    "Link prediction allows us to infer missing links in a graph. This has many real-world use cases, such as predicting connections between people in a social network, interactions between proteins in a biological network, and music recommendation based on prior user taste.\n",
    "\n",
    "In our case, we are going to see which of the following candidate statements are more likely to be true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unseen = np.array([\n",
    "    ['/players/t/thompkl01.html', 'has_similar_playing_pattern', '/players/c/curryst01.html'],\n",
    "    ['/players/b/bryanko01.html', 'has_similar_playing_pattern', '/players/w/wadedw01.html'],\n",
    "    ['/players/t/thompkl01.html', 'has_similar_playing_pattern', '/players/b/bryanko01.html'],\n",
    "    ['/players/a/anthoca01.html', 'has_similar_playing_pattern', '/players/j/jamesle01.html']\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_filter = np.array(list({tuple(i) for i in np.vstack((positives_filter, X_unseen))}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 25.61it/s]\n"
     ]
    }
   ],
   "source": [
    "ranks_unseen = evaluate_performance(\n",
    "    X_unseen, \n",
    "    model=model, \n",
    "    filter_triples=unseen_filter,\n",
    "    corrupt_side = 's+o',\n",
    "    use_default_protocol=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict(X_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform the scores (real numbers) into probabilities (bound between 0 and 1) using the `expit` transform (note that the probabilities are not calibrated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "probs = expit(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.35542285], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>rank</th>\n",
       "      <th>score</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/players/b/bryanko01.html has_similar_playing_...</td>\n",
       "      <td>1</td>\n",
       "      <td>10.095822</td>\n",
       "      <td>0.999959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/players/a/anthoca01.html has_similar_playing_...</td>\n",
       "      <td>195</td>\n",
       "      <td>1.910819</td>\n",
       "      <td>0.871111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/players/t/thompkl01.html has_similar_playing_...</td>\n",
       "      <td>4150</td>\n",
       "      <td>-0.595286</td>\n",
       "      <td>0.355423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/players/t/thompkl01.html has_similar_playing_...</td>\n",
       "      <td>3783</td>\n",
       "      <td>-0.616620</td>\n",
       "      <td>0.350551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement  rank      score  \\\n",
       "1  /players/b/bryanko01.html has_similar_playing_...     1  10.095822   \n",
       "3  /players/a/anthoca01.html has_similar_playing_...   195   1.910819   \n",
       "0  /players/t/thompkl01.html has_similar_playing_...  4150  -0.595286   \n",
       "2  /players/t/thompkl01.html has_similar_playing_...  3783  -0.616620   \n",
       "\n",
       "       prob  \n",
       "1  0.999959  \n",
       "3  0.871111  \n",
       "0  0.355423  \n",
       "2  0.350551  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(list(zip([' '.join(x) for x in X_unseen], \n",
    "                      ranks_unseen, \n",
    "                      np.squeeze(scores),\n",
    "                      np.squeeze(probs))), \n",
    "             columns=['statement', 'rank', 'score', 'prob']).sort_values(\"score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1.x.3: Visualizing Embeddings with Tensorboard projector\n",
    "\n",
    "we can now visualize the high-dimensional embeddings in the browser. Lets import the `create_tensorboard_visualization` function, which simplifies the creation of the files necessary for Tensorboard to display the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ampligraph.utils import create_tensorboard_visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we'll run the function with our model, specifying the output path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tensorboard_visualizations(model, 'dsci558_embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, we should now have a number of files a directory called `dsci558_embeddings`.\n",
    "To visualize the embeddings in Tensorboard, go to (`cd`) `dsci558_embeddings` and run the following command: `tensorboard --logdir=./visualizations`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**^ Please note that a screenshot of embedding visualization is required for task 2.1.x.3 ^**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "USC-Knowledge-Graph",
   "language": "python",
   "name": "usc-knowledge-graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
